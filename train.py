# -*- coding: utf-8 -*-
"""Assignment-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/anuragbantu/DL-Assignment-1/blob/main/Assignment-1.ipynb
"""

#import the required libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from keras.datasets import fashion_mnist

"""# **Question 1**

The fashion_MNIST dataset is loaded and 1 sample image for each classs is plotted below.
"""

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

#installing wandb and then logging in
!pip install wandb -Uq

import wandb
wandb.login()

class_labels = {
    0: 'T-shirt/top',
    1: 'Trouser',
    2: 'Pullover',
    3: 'Dress',
    4: 'Coat',
    5: 'Sandal',
    6: 'Shirt',
    7: 'Sneaker',
    8: 'Bag',
    9: 'Ankle boot'
}

wandb.init(project="fashion-mnist-visualization")

fig, axes = plt.subplots(4, 3, figsize=(8, 8))

for i in range(10):
    idx = np.where(y_train == i)[0][0]
    ax = axes[i // 3, i % 3]
    ax.imshow(x_train[idx], cmap='gray')
    ax.set_title(class_labels[i])
    ax.axis('off')

plt.tight_layout()

wandb.log({"Fashion MNIST Samples": wandb.Image(fig)})

plt.show()

wandb.finish()

"""## **Question 2 & 3**

preparing the dataset.
"""

x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_train = x_train.astype('float32') / 255.0

def one_hot_encode(labels, num_classes=None):
    if num_classes is None:
        num_classes = np.max(labels) + 1

    encoded = np.zeros((len(labels), num_classes))

    encoded[np.arange(len(labels)), labels] = 1

    return encoded

y_train = one_hot_encode(y_train)

y_test = one_hot_encode(y_test)

"""Define all the activation functions and the necessary loss functions. We will use cross entropy loss for the most part but in Question 8 we will use squared error loss as well."""

def sigmoid(x):
  return 0.5 * (1 + np.tanh(0.5 * x))

def sigmoid_d(x):
  s= sigmoid(x)
  return s*(1-s)

def softmax(x):
    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True) + 1e-8)
    return e_x / np.sum(e_x, axis=-1, keepdims=True)

def softmax_d(x):
    diag_s = np.diag(x)

    outer_s = np.outer(x, x)
    return diag_s - outer_s

def tanh(x):
    return np.tanh(x)

def tanh_d(x):
    return 1 - np.square(np.tanh(x))

def relu(x):
    return np.maximum(0, x)

def relu_d(x):
    return np.where(x > 0, 1, 0)

#CCE loss function
def categorical_cross_entropy(y_true, y_pred, layers, weight_decay=0):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    loss = -np.sum(y_true * np.log(y_pred), axis=-1)
    loss = np.mean(loss)

    l2_penalty = weight_decay * sum(np.sum(layer.weights ** 2) for layer in layers)

    return loss + l2_penalty

#MSE loss
def squared_error_loss(y_true, y_pred):
    return (np.mean(np.sum((y_true - y_pred) ** 2, axis=1))/2)

activations = {"sigmoid":sigmoid,"softmax":softmax,"tanh":tanh,"relu":relu}

derivatives = {"sigmoid":sigmoid_d}

"""Define a class for layers."""

class layer:
  def __init__(self, input, neurons, activation, weight_init):
    if weight_init == "xavier":
        self.weights = np.random.randn(input, neurons) * np.sqrt(1 / input)
    else:
        self.weights = np.random.randn(input, neurons)
    self.bias = np.zeros((1, neurons))
    self.activation = activations[activation]
    self.activation_d = derivatives.get(activation, None)

  #define forward propagation
  def forward(self, a_prev):
    self.a_prev = a_prev
    self.z = np.dot(self.a_prev,self.weights) + self.bias
    self.a = self.activation(self.z)
    return self.a

  #define all the optimization techniques being used
  def sgd(self,w_d,b_d,lr):
    self.weights -= lr*w_d
    self.bias -= lr*b_d

  def momentum_gd(self, w_d, b_d, lr, momentum=0.9):
    if not hasattr(self, 'm_w'):
        self.m_w = np.zeros_like(self.weights)
    if not hasattr(self, 'm_b'):
        self.m_b = np.zeros_like(self.bias)
    self.m_w = momentum * self.m_w + lr * w_d
    self.m_b = momentum * self.m_b + lr * b_d
    self.weights -= self.m_w
    self.bias -= self.m_b

  def nesterov_gd(self, w_d, b_d, lr, momentum=0.9):
      if not hasattr(self, 'm_w'):
          self.m_w = np.zeros_like(self.weights)
      if not hasattr(self, 'm_b'):
          self.m_b = np.zeros_like(self.bias)
      m_w_prev, m_b_prev = self.m_w, self.m_b
      self.m_w = momentum * self.m_w + lr * w_d
      self.m_b = momentum * self.m_b + lr * b_d
      self.weights -= momentum * m_w_prev + (1 + momentum) * self.m_w
      self.bias -= momentum * m_b_prev + (1 + momentum) * self.m_b

  def rmsprop(self, w_d, b_d, lr, decay_rate=0.9, epsilon=1e-8):
      if not hasattr(self, 'v_w'):
          self.v_w = np.zeros_like(self.weights)
      if not hasattr(self, 'v_b'):
          self.v_b = np.zeros_like(self.bias)
      self.v_w = decay_rate * self.v_w + (1 - decay_rate) * np.square(w_d)
      self.v_b = decay_rate * self.v_b + (1 - decay_rate) * np.square(b_d)
      self.weights -= lr * w_d / (np.sqrt(self.v_w) + epsilon)
      self.bias -= lr * b_d / (np.sqrt(self.v_b) + epsilon)

  def adam(self, w_d, b_d, lr, beta1=0.9, beta2=0.999, epsilon=1e-8):

      if not hasattr(self, 'm_w'):
        self.m_w = np.zeros_like(self.weights)
      if not hasattr(self, 'm_b'):
          self.m_b = np.zeros_like(self.bias)
      if not hasattr(self, 'v_w'):
          self.v_w = np.zeros_like(self.weights)
      if not hasattr(self, 'v_b'):
          self.v_b = np.zeros_like(self.bias)
      if not hasattr(self, 't'):
          self.t = 0

      self.t += 1

      self.m_w = beta1 * self.m_w + (1 - beta1) * w_d
      self.m_b = beta1 * self.m_b + (1 - beta1) * b_d
      self.v_w = beta2 * self.v_w + (1 - beta2) * np.square(w_d)
      self.v_b = beta2 * self.v_b + (1 - beta2) * np.square(b_d)
      m_w_hat = self.m_w / (1 - beta1**self.t)
      m_b_hat = self.m_b / (1 - beta1**self.t)
      v_w_hat = self.v_w / (1 - beta2**self.t)
      v_b_hat = self.v_b / (1 - beta2**self.t)
      self.weights -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
      self.bias -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

  def nadam(self, w_d, b_d, lr, beta1=0.9, beta2=0.999, epsilon=1e-8):
      if not hasattr(self, 'm_w'):
        self.m_w = np.zeros_like(self.weights)
      if not hasattr(self, 'm_b'):
          self.m_b = np.zeros_like(self.bias)
      if not hasattr(self, 'v_w'):
          self.v_w = np.zeros_like(self.weights)
      if not hasattr(self, 'v_b'):
          self.v_b = np.zeros_like(self.bias)
      if not hasattr(self, 't'):
          self.t = 0
      self.t += 1
      self.m_w = beta1 * self.m_w + (1 - beta1) * w_d
      self.m_b = beta1 * self.m_b + (1 - beta1) * b_d
      self.v_w = beta2 * self.v_w + (1 - beta2) * np.square(w_d)
      self.v_b = beta2 * self.v_b + (1 - beta2) * np.square(b_d)
      m_w_hat = self.m_w / (1 - beta1**self.t)
      m_b_hat = self.m_b / (1 - beta1**self.t)
      v_w_hat = self.v_w / (1 - beta2**self.t)
      v_b_hat = self.v_b / (1 - beta2**self.t)
      m_w_bar = beta1 * m_w_hat + ((1 - beta1) / (1 - beta1**self.t)) * w_d
      m_b_bar = beta1 * m_b_hat + ((1 - beta1) / (1 - beta1**self.t)) * b_d
      self.weights -= lr * m_w_bar / (np.sqrt(v_w_hat) + epsilon)
      self.bias -= lr * m_b_bar / (np.sqrt(v_b_hat) + epsilon)


  #define the backpropagation algorithm
  def backprop(self, a_d, lr, optimizer, weight_decay=0):
    if self.activation_d:
      z_d = np.multiply(self.activation_d(self.z),a_d)
    else:
      z_d = a_d

    w_d = np.dot(self.a_prev.T,z_d) + weight_decay * self.weights
    b_d = np.sum(z_d,axis=0,keepdims=True)
    a_prev_d = np.dot(z_d,self.weights.T)

    if optimizer == 'sgd':
            self.sgd(w_d, b_d, lr)
    elif optimizer == 'momentum':
        self.momentum_gd(w_d, b_d, lr)
    elif optimizer == 'nesterov':
        self.nesterov_gd(w_d, b_d, lr)
    elif optimizer == 'rmsprop':
        self.rmsprop(w_d, b_d, lr)
    elif optimizer == 'adam':
        self.adam(w_d, b_d, lr)
    elif optimizer == 'nadam':
        self.nadam(w_d, b_d, lr)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer}")

    return a_prev_d

"""# Question 3

Implement backpropagation algorithm

# Question 4,5,6

Training different neural entwrok models while using wandb for hyperparamter tuning to find the best performing combinations.
"""

#setting 10%of the training data aside for validation
from sklearn.model_selection import train_test_split

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=0)

"""Setting up the strategy and different parameter values to be analyzed by wandb sweep experiments. Here, we use Bayesian optimization with the goal of maximizing validation accuracy."""

#setting the sweep configuration for wandb
import numpy as np

sweep_config = {
    'method': 'bayes',
    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},
    'parameters': {
        'epochs': {'values': [5, 10, 15]},
        'num_hidden_layers': {'values': [3, 4, 5]},
        'fc_layer_size': {'values': [32, 64, 128]},
        'weight_decay': {'values': [0, 0.0005,0.005,0.05, 0.5]},
        'lr': {'values': [1e-3, 1e-4,1e-5]},
        'optimizer': {'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},
        'batch_size': {'values': [16, 32, 64]},
        'weight_init': {'values': ['random', 'xavier']},
        'activation': {'values': ['sigmoid', 'tanh', 'relu']}
    }
}

sweep_id = wandb.sweep(sweep_config, project="neural-network-hyperparam-tuning")

#defining accuracy metric to measure the model performance
def accuracy(y_true, y_pred):
    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))

"""Now, we train the neural networks with different hyperparameter values on the training set. After each epoch, we analyze the performance of the model on the validation set and at the end after all the epochs of training the model we look at the performance on the test dataset. This wandb sweep is run with count=100 which will evaluate 100 different combinations of hyperparameters."""

#train the neural network with categorical cross entropy loss

def train_network(config=None):
    with wandb.init(config=config, project="neural-network-hyperparam-tuning"):
        config = wandb.config

        #set the model name using all the parameter values
        run_name = (f"hl_{config['num_hidden_layers']}_bs_{config['batch_size']}_ac_{config['activation']}_"
            f"fc_{config['fc_layer_size']}_lr_{config['lr']}_opt_{config['optimizer']}_"
            f"wd_{config['weight_decay']}_wi_{config['weight_init']}_ep_{config['epochs']}")
        wandb.run.name = run_name

        #initialize the 1st input layer
        layers = [
            layer(784, config.fc_layer_size, config.activation, config.weight_init)
        ]


        #build the subsequent hidden layers and add the last outputlayer
        for _ in range(config.num_hidden_layers - 1):
            layers.append(layer(config.fc_layer_size, config.fc_layer_size, config.activation, config.weight_init))
        layers.append(layer(config.fc_layer_size, 10, "softmax", config.weight_init))

        num_samples = x_train.shape[0]

        #train over epochs
        for epoch in range(config.epochs):
            shuffled_indices = np.random.permutation(num_samples)
            x_train_shuffled = x_train[shuffled_indices]
            y_train_shuffled = y_train[shuffled_indices]

            epoch_loss = 0
            epoch_acc = 0

            #iterate over batches
            for start in range(0, num_samples, config.batch_size):
                end = min(start + config.batch_size, num_samples)
                x_batch = x_train_shuffled[start:end]
                y_batch = y_train_shuffled[start:end]

                #forward propagation
                a = x_batch
                for l in layers:
                    a = l.forward(a)

                #calculate loss and accuracy
                loss = categorical_cross_entropy(y_batch, a,layers, config.weight_decay)
                acc = accuracy(y_batch, a)

                epoch_loss += loss
                epoch_acc += acc

                #backward propagation
                a_d = a - y_batch
                for l in reversed(layers):
                    a_d = l.backprop(a_d, config.lr, config.optimizer, config.weight_decay)

            num_batches = max(1, num_samples // config.batch_size)
            avg_loss = epoch_loss / num_batches
            avg_acc = epoch_acc / num_batches

            #calculate the validation loss and accuracy for the epoch
            a_val = x_val
            for l in layers:
                a_val = l.forward(a_val)

            val_loss = categorical_cross_entropy(y_val, a_val, layers, config.weight_decay)
            val_acc = accuracy(y_val, a_val)

            print(f"Epoch {epoch + 1}, Loss: {avg_loss}, Accuracy: {avg_acc}, val_loss: {val_loss}, val_accuracy: {val_acc}")

            wandb.log({"epoch": epoch + 1, "loss": avg_loss, "accuracy": avg_acc,
                       "val_loss": val_loss, "val_accuracy": val_acc})

        #After training, evaluate the model on the test dataset
        a_test = x_test
        for l in layers:
            a_test = l.forward(a_test)

        test_loss = categorical_cross_entropy(y_test, a_test,layers, config.weight_decay)
        test_acc = accuracy(y_test, a_test)
        print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

        wandb.log({"test_loss": test_loss, "test_accuracy": test_acc})


        #plot the confusion matrices
        y_pred = np.argmax(a_test, axis=1)
        y_true = np.argmax(y_test, axis=1)
        cm = confusion_matrix(y_true, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.title("Confusion Matrix")

        wandb.log({"confusion_matrix": wandb.Image(plt)})
        plt.close()



x_train = x_train.reshape(x_train.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

wandb.agent(sweep_id, train_network, count=5) #used count=100 for the main sweep

wandb.finish()

"""# Question 7

We manually observe the best performing model from the sweep analysis charts based on maximizing test accuracy and evaluate the model again looking at the resulting confusing matrix of the model predictions.
"""

#best model for cross entropy


wandb.init(project="neural-network-hyperparam-tuning", name="best_model_evaluation_0")

#set the identified best configuration
best_config = {
    "activation": "sigmoid",
    "batch_size": 16,
    "epochs": 15,
    "fc_layer_size": 128,
    "lr": 0.001,
    "num_hidden_layers": 3,
    "optimizer": "nadam",
    "weight_decay": 0.0005,
    "weight_init": "xavier"
}

#buil the neural network
layers = [
    layer(784, best_config["fc_layer_size"], best_config["activation"], best_config["weight_init"])
]
for _ in range(best_config["num_hidden_layers"] - 1):
    layers.append(layer(best_config["fc_layer_size"], best_config["fc_layer_size"], best_config["activation"], best_config["weight_init"]))
layers.append(layer(best_config["fc_layer_size"], 10, "softmax", best_config["weight_init"]))

num_samples = x_train.shape[0]

#train over epochs
for epoch in range(best_config["epochs"]):
    shuffled_indices = np.random.permutation(num_samples)
    x_train_shuffled = x_train[shuffled_indices]
    y_train_shuffled = y_train[shuffled_indices]

    epoch_loss = 0
    epoch_acc = 0
    #iterate over batches
    for start in range(0, num_samples, best_config["batch_size"]):
        end = min(start + best_config["batch_size"], num_samples)
        x_batch = x_train_shuffled[start:end]
        y_batch = y_train_shuffled[start:end]

        #perform forward propagation
        a = x_batch
        for l in layers:
            a = l.forward(a)
        #calucate the loss and accuracy
        loss = categorical_cross_entropy(y_batch, a, layers, best_config["weight_decay"])
        acc = accuracy(y_batch, a)

        epoch_loss += loss
        epoch_acc += acc

        #perform backward propagation
        a_d = a - y_batch
        for l in reversed(layers):
            a_d = l.backprop(a_d, best_config["lr"], best_config["optimizer"])

    num_batches = max(1, num_samples // best_config["batch_size"])
    avg_loss = epoch_loss / num_batches
    avg_acc = epoch_acc / num_batches

    print(f"Epoch {epoch + 1}, Loss: {avg_loss}, Accuracy: {avg_acc}")

    wandb.log({"epoch": epoch + 1, "loss": avg_loss, "accuracy": avg_acc})


#analyze performance on the test data
a_test = x_test
for l in layers:
    a_test = l.forward(a_test)

test_loss = categorical_cross_entropy(y_test, a_test, layers, best_config["weight_decay"])
test_acc = accuracy(y_test, a_test)

print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

wandb.log({"test_loss": test_loss, "test_accuracy": test_acc})

#build a confusion matric
y_pred = np.argmax(a_test, axis=1)
y_true = np.argmax(y_test, axis=1)
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix of Best Model")

wandb.log({"confusion_matrix": wandb.Image(plt)})
plt.show()

wandb.finish()

"""# Question 8

Now, instead of using cross entropy loss, we will use the squared error loss function and perform a similar wandb sweep as before to identify the best models and the accuracies achieved with this loss function.
"""

#with MSE loss function

#define new sweep
new_sweep_id = wandb.sweep(sweep_config, project="neural-network-hyperparam-tuning")


def train_network(config=None):
    with wandb.init(config=config, project="neural-network-hyperparam-tuning"):
        config = wandb.config
        run_name = f"hl_{config.num_hidden_layers}_bs_{config.batch_size}_ac_{config.activation}"
        wandb.run.name = run_name

        layers = [
            layer(784, config.fc_layer_size, config.activation, config.weight_init)
        ]

        for _ in range(config.num_hidden_layers - 1):
            layers.append(layer(config.fc_layer_size, config.fc_layer_size, config.activation, config.weight_init))
        layers.append(layer(config.fc_layer_size, 10, "softmax", config.weight_init))

        num_samples = x_train.shape[0]

        for epoch in range(config.epochs):
            shuffled_indices = np.random.permutation(num_samples)
            x_train_shuffled = x_train[shuffled_indices]
            y_train_shuffled = y_train[shuffled_indices]

            epoch_loss = 0
            epoch_acc = 0

            for start in range(0, num_samples, config.batch_size):
                end = min(start + config.batch_size, num_samples)
                x_batch = x_train_shuffled[start:end]
                y_batch = y_train_shuffled[start:end]

                a = x_batch
                for l in layers:
                    a = l.forward(a)

                loss = squared_error_loss(y_batch, a)
                acc = accuracy(y_batch, a)

                epoch_loss += loss
                epoch_acc += acc

                a_d = (a - y_batch) * a * (1 - a)   #derivative of MSE loss with softmax output layer
                for l in reversed(layers):
                    a_d = l.backprop(a_d, config.lr, config.optimizer)

            num_batches = max(1, num_samples // config.batch_size)
            avg_loss = epoch_loss / num_batches
            avg_acc = epoch_acc / num_batches

            a_val = x_val
            for l in layers:
                a_val = l.forward(a_val)

            val_loss = squared_error_loss(y_val, a_val)
            val_acc = accuracy(y_val, a_val)

            print(f"Epoch {epoch + 1}, Loss: {avg_loss}, Accuracy: {avg_acc}, val_loss: {val_loss}, val_accuracy: {val_acc}")

            wandb.log({"epoch": epoch + 1, "loss": avg_loss, "accuracy": avg_acc,
                       "val_loss": val_loss, "val_accuracy": val_acc})

        a_test = x_test
        for l in layers:
            a_test = l.forward(a_test)

        test_loss = squared_error_loss(y_test, a_test)
        test_acc = accuracy(y_test, a_test)
        print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

        wandb.log({"test_loss": test_loss, "test_accuracy": test_acc})

        y_pred = np.argmax(a_test, axis=1)
        y_true = np.argmax(y_test, axis=1)
        cm = confusion_matrix(y_true, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.title("Confusion Matrix")

        wandb.log({"confusion_matrix": wandb.Image(plt)})
        plt.close()



x_train = x_train.reshape(x_train.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

wandb.agent(new_sweep_id, train_network, count=5) #used count = 50 for the main sweep

wandb.finish()

"""As we did for cross entropy, we can analyze the best performing model for the squared error loss function."""

#best model for MSE loss function

wandb.init(project="neural-network-hyperparam-tuning", name="best_model_evaluation")

best_config = {
    "activation": "sigmoid",
    "batch_size": 64,
    "epochs": 10,
    "fc_layer_size": 64,
    "lr": 0.001,
    "num_hidden_layers": 4,
    "optimizer": "nadam",
    "weight_decay": 0.0005,
    "weight_init": "random"
}

layers = [
    layer(784, best_config["fc_layer_size"], best_config["activation"], best_config["weight_init"])
]
for _ in range(best_config["num_hidden_layers"] - 1):
    layers.append(layer(best_config["fc_layer_size"], best_config["fc_layer_size"], best_config["activation"], best_config["weight_init"]))
layers.append(layer(best_config["fc_layer_size"], 10, "softmax", best_config["weight_init"]))

num_samples = x_train.shape[0]

for epoch in range(best_config["epochs"]):
    shuffled_indices = np.random.permutation(num_samples)
    x_train_shuffled = x_train[shuffled_indices]
    y_train_shuffled = y_train[shuffled_indices]

    epoch_loss = 0
    epoch_acc = 0

    for start in range(0, num_samples, best_config["batch_size"]):
        end = min(start + best_config["batch_size"], num_samples)
        x_batch = x_train_shuffled[start:end]
        y_batch = y_train_shuffled[start:end]

        a = x_batch
        for l in layers:
            a = l.forward(a)

        loss = squared_error_loss(y_batch, a)
        acc = accuracy(y_batch, a)

        epoch_loss += loss
        epoch_acc += acc

        a_d = a - y_batch
        for l in reversed(layers):
            a_d = l.backprop(a_d, best_config["lr"], best_config["optimizer"])

    num_batches = max(1, num_samples // best_config["batch_size"])
    avg_loss = epoch_loss / num_batches
    avg_acc = epoch_acc / num_batches

    print(f"Epoch {epoch + 1}, Loss: {avg_loss}, Accuracy: {avg_acc}")

    wandb.log({"epoch": epoch + 1, "loss": avg_loss, "accuracy": avg_acc})


a_test = x_test
for l in layers:
    a_test = l.forward(a_test)

test_loss = squared_error_loss(y_test, a_test)
test_acc = accuracy(y_test, a_test)

print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

wandb.log({"test_loss": test_loss, "test_accuracy": test_acc})

y_pred = np.argmax(a_test, axis=1)
y_true = np.argmax(y_test, axis=1)
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix of Best Model")

wandb.log({"confusion_matrix": wandb.Image(plt)})
plt.show()

wandb.finish()

"""# Question 10

Based on our learnings from the experiments on the Fashion-MNIST dataset, we will take 3 of the best performing models and evaluate their performance on the MNIST dataset
"""

from tensorflow.keras.datasets import mnist

(x_1_train, y_1_train), (x_1_test, y_1_test) = mnist.load_data()

x_1_train = x_1_train.reshape(x_1_train.shape[0], 28, 28, 1)
x_1_train = x_1_train.astype('float32') / 255.0

y_1_train = one_hot_encode(y_1_train)
y_1_test = one_hot_encode(y_1_test)

x_train.shape

x_1_train.shape

rec_config_1 = {
    "activation": "sigmoid",
    "batch_size": 16,
    "epochs": 15,
    "fc_layer_size": 128,
    "lr": 0.001,
    "num_hidden_layers": 3,
    "optimizer": "nadam",
    "weight_decay": 0.005,
    "weight_init": "xavier"
}

layers = [layer(784, rec_config_1["fc_layer_size"], rec_config_1["activation"], rec_config_1["weight_init"])]
for _ in range(rec_config_1["num_hidden_layers"] - 1):
    layers.append(layer(rec_config_1["fc_layer_size"], rec_config_1["fc_layer_size"], rec_config_1["activation"], rec_config_1["weight_init"]))
layers.append(layer(rec_config_1["fc_layer_size"], 10, "softmax", rec_config_1["weight_init"]))

num_samples = x_1_train.shape[0]

x_1_train = x_1_train.reshape(x_1_train.shape[0], -1)
x_1_test = x_1_test.reshape(x_1_test.shape[0], -1)


for epoch in range(rec_config_1["epochs"]):
    shuffled_indices = np.random.permutation(num_samples)
    x_1_train_shuffled = x_1_train[shuffled_indices]
    y_1_train_shuffled = y_1_train[shuffled_indices]

    for start in range(0, num_samples, rec_config_1["batch_size"]):
        end = min(start + rec_config_1["batch_size"], num_samples)
        x_batch = x_1_train_shuffled[start:end]
        y_batch = y_1_train_shuffled[start:end]

        a = x_batch
        for l in layers:
            a = l.forward(a)

        loss = categorical_cross_entropy(y_batch, a, layers, rec_config_1["weight_decay"])

        a_d = a - y_batch
        for l in reversed(layers):
            a_d = l.backprop(a_d, rec_config_1["lr"], rec_config_1["optimizer"])

a_test = x_1_test
for l in layers:
    a_test = l.forward(a_test)

acc = accuracy(y_1_test, a_test)

print(f"Accuracy on x_1_test: {acc}")
print("Predictions on x_1_test:", y_pred)

rec_config_2 = {
    "activation": "sigmoid",
    "batch_size": 32,
    "epochs": 5,
    "fc_layer_size": 128,
    "lr": 0.001,
    "num_hidden_layers": 3,
    "optimizer": "adam",
    "weight_decay": 0.005,
    "weight_init": "xavier"
}

layers_2 = [layer(784, rec_config_2["fc_layer_size"], rec_config_2["activation"], rec_config_2["weight_init"])]
for _ in range(rec_config_2["num_hidden_layers"] - 1):
    layers_2.append(layer(rec_config_2["fc_layer_size"], rec_config_2["fc_layer_size"], rec_config_2["activation"], rec_config_2["weight_init"]))
layers_2.append(layer(rec_config_2["fc_layer_size"], 10, "softmax", rec_config_2["weight_init"]))

num_samples_2 = x_1_train.shape[0]

x_1_train = x_1_train.reshape(x_1_train.shape[0], -1)
x_1_test = x_1_test.reshape(x_1_test.shape[0], -1)

for epoch in range(rec_config_2["epochs"]):
    shuffled_indices = np.random.permutation(num_samples_2)
    x_1_train_shuffled = x_1_train[shuffled_indices]
    y_1_train_shuffled = y_1_train[shuffled_indices]

    for start in range(0, num_samples_2, rec_config_2["batch_size"]):
        end = min(start + rec_config_2["batch_size"], num_samples_2)
        x_batch = x_1_train_shuffled[start:end]
        y_batch = y_1_train_shuffled[start:end]

        a = x_batch
        for l in layers_2:
            a = l.forward(a)

        loss = categorical_cross_entropy(y_batch, a, layers_2, rec_config_2["weight_decay"])

        a_d = a - y_batch
        for l in reversed(layers_2):
            a_d = l.backprop(a_d, rec_config_2["lr"], rec_config_2["optimizer"])

a_test = x_1_test
for l in layers_2:
    a_test = l.forward(a_test)

acc = accuracy(y_1_test, a_test)

print(f"Accuracy on x_1_test for rec_config_2: {acc}")
print("Predictions on x_1_test for rec_config_2:", np.argmax(a_test, axis=1))

rec_config_3 = {
    "activation": "relu",
    "batch_size": 64,
    "epochs": 5,
    "fc_layer_size": 32,
    "lr": 0.001,
    "num_hidden_layers": 3,
    "optimizer": "adam",
    "weight_decay": 0.005,
    "weight_init": "xavier"
}

layers_3 = [layer(784, rec_config_3["fc_layer_size"], rec_config_3["activation"], rec_config_3["weight_init"])]
for _ in range(rec_config_3["num_hidden_layers"] - 1):
    layers_3.append(layer(rec_config_3["fc_layer_size"], rec_config_3["fc_layer_size"], rec_config_3["activation"], rec_config_3["weight_init"]))
layers_3.append(layer(rec_config_3["fc_layer_size"], 10, "softmax", rec_config_3["weight_init"]))

num_samples_3 = x_1_train.shape[0]

x_1_train = x_1_train.reshape(x_1_train.shape[0], -1)
x_1_test = x_1_test.reshape(x_1_test.shape[0], -1)

for epoch in range(rec_config_3["epochs"]):
    shuffled_indices = np.random.permutation(num_samples_3)
    x_1_train_shuffled = x_1_train[shuffled_indices]
    y_1_train_shuffled = y_1_train[shuffled_indices]

    for start in range(0, num_samples_3, rec_config_3["batch_size"]):
        end = min(start + rec_config_3["batch_size"], num_samples_3)
        x_batch = x_1_train_shuffled[start:end]
        y_batch = y_1_train_shuffled[start:end]

        a = x_batch
        for l in layers_3:
            a = l.forward(a)

        loss = categorical_cross_entropy(y_batch, a, layers_3, rec_config_3["weight_decay"])

        a_d = a - y_batch
        for l in reversed(layers_3):
            a_d = l.backprop(a_d, rec_config_3["lr"], rec_config_3["optimizer"])

a_test = x_1_test
for l in layers_3:
    a_test = l.forward(a_test)

acc = accuracy(y_1_test, a_test)

print(f"Accuracy on x_1_test for rec_config_3: {acc}")
print("Predictions on x_1_test for rec_config_3:", np.argmax(a_test, axis=1))
